{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5feda89b",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd54317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from statsmodels.stats.contingency_tables import mcnemar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c142f6",
   "metadata": {},
   "source": [
    "#### Check if CUDA is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "027d7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf125e",
   "metadata": {},
   "source": [
    "## Define NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7be83e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularPropertyPrediction(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MolecularPropertyPrediction, self).__init__()\n",
    "        self.embed_atom = nn.Embedding(n_fingerprint, dim)\n",
    "        self.W_atom = nn.ModuleList([nn.Linear(dim, dim) for _ in range(layer)])\n",
    "        self.W_property = nn.Linear(dim+extra_dim, 11)\n",
    "    \n",
    "    \"\"\"Pad adjacency matrices for batch processing.\"\"\"\n",
    "    def pad(self, matrices, value):\n",
    "        sizes = [d.shape[0] for d in matrices]\n",
    "        D = sum(sizes)\n",
    "        pad_matrices = value + np.zeros((D, D))\n",
    "        m = 0\n",
    "        for i, d in enumerate(matrices):\n",
    "            s_i = sizes[i]\n",
    "            pad_matrices[m:m+s_i, m:m+s_i] = d\n",
    "            m += s_i\n",
    "        return torch.FloatTensor(pad_matrices).to(device)\n",
    "    \n",
    "    def sum_axis(self, xs, axis):\n",
    "        y = list(map(lambda x: torch.sum(x, 0), torch.split(xs, axis)))\n",
    "        return torch.stack(y)\n",
    "    \n",
    "    def update(self, xs, adjacency, i):\n",
    "        hs = torch.relu(self.W_atom[i](xs))\n",
    "        return torch.matmul(adjacency, hs)\n",
    "    \n",
    "    def forward(self, inputs, sel_maccs):\n",
    "        \n",
    "        atoms, adjacency = inputs\n",
    "        axis = list(map(lambda x: len(x), atoms))\n",
    "\n",
    "        atoms = torch.cat(atoms)\n",
    "        x_atoms = self.embed_atom(atoms)\n",
    "        adjacency = self.pad(adjacency, 0)\n",
    "\n",
    "        for i in range(layer):\n",
    "            x_atoms = self.update(x_atoms, adjacency, i)\n",
    "        \n",
    "        extra_inputs = sel_maccs.to(device)\n",
    "        y_molecules = self.sum_axis(x_atoms, axis)\n",
    "        \n",
    "        #AM: Enable this if we want extra_inputs.\n",
    "        y_molecules = torch.cat((y_molecules,extra_inputs),1)\n",
    "        z_properties = self.W_property(y_molecules)\n",
    "        \n",
    "        return z_properties\n",
    "    \n",
    "    def embed_inputs(self, inputs, sel_maccs):\n",
    "        atoms, adjacency = inputs\n",
    "        axis = list(map(lambda x: len(x), atoms))\n",
    "\n",
    "        atoms = torch.cat(atoms)\n",
    "        x_atoms = self.embed_atom(atoms)\n",
    "        adjacency = self.pad(adjacency, 0)\n",
    "\n",
    "        for i in range(layer):\n",
    "            x_atoms = self.update(x_atoms, adjacency, i)\n",
    "        \n",
    "        extra_inputs = sel_maccs.to(device)\n",
    "        y_molecules = self.sum_axis(x_atoms, axis)\n",
    "        \n",
    "        y_molecules = torch.cat((y_molecules,extra_inputs),1)\n",
    "        return(y_molecules)    \n",
    "    \n",
    "    \n",
    "    def __call__(self, data_batch, train=True, correlation_analysis=False):\n",
    "        \n",
    "        sel_maccs = torch.FloatTensor(data_batch[-1])\n",
    "        \n",
    "        inputs, t_properties = data_batch[:-2], torch.cat(data_batch[-2])\n",
    "        \n",
    "        if correlation_analysis:\n",
    "            embedding = self.embed_inputs(inputs, sel_maccs)\n",
    "            return(embedding)\n",
    "        \n",
    "        z_properties = self.forward(inputs, sel_maccs)\n",
    "        \n",
    "        if train:\n",
    "            loss = F.cross_entropy(z_properties, t_properties)\n",
    "            return loss\n",
    "        else:\n",
    "            zs = F.softmax(z_properties, 1).to('cpu').data.numpy()\n",
    "            ts = t_properties.to('cpu').data.numpy()\n",
    "            scores = list(map(lambda x: x.argsort()[-3:][::-1], zs))\n",
    "            labels = list(map(lambda x: np.argmax(x), zs))\n",
    "            return scores, labels, ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007f273",
   "metadata": {},
   "source": [
    "#### Create trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315aac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    def train(self, dataset_train):\n",
    "        np.random.shuffle(dataset_train)\n",
    "        N = len(dataset_train)\n",
    "        loss_total = 0\n",
    "        for i in range(0, N, batch):\n",
    "            data_batch = list(zip(*dataset_train[i:i+batch]))\n",
    "            loss = self.model(data_batch)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss_total += loss.to('cpu').data.numpy()\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815db07",
   "metadata": {},
   "source": [
    "#### Create tester class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34427674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester(object):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def test(self, dataset_test):\n",
    "\n",
    "        N = len(dataset_test)\n",
    "        score_list, label_list, t_list = [], [], []\n",
    "\n",
    "        for i in range(0, N, batch):\n",
    "            data_batch = list(zip(*dataset_test[i:i+batch]))\n",
    "            scores, labels, ts = self.model(data_batch, train=False)\n",
    "            score_list = np.append(score_list, scores)\n",
    "            label_list = np.append(label_list, labels)\n",
    "            t_list = np.append(t_list, ts)\n",
    "        \n",
    "        neg_pos_list1 = []\n",
    "        neg_pos_list2 = []\n",
    "        neg_pos_list3 = []\n",
    "        auc = accuracy_score(t_list, label_list)\n",
    "        \n",
    "        auc2, auc3 = 0, 0 \n",
    "        for i in range(0, N):\n",
    "            true_class = t_list[i]\n",
    "            \n",
    "            if true_class == score_list[3*i]:\n",
    "                neg_pos_list1.append(1)\n",
    "            else:\n",
    "                neg_pos_list1.append(0)\n",
    "            \n",
    "            if true_class in score_list[3*i:3*i+2]:\n",
    "                auc2 += 1\n",
    "                neg_pos_list2.append(1)\n",
    "            else:\n",
    "                neg_pos_list2.append(0)\n",
    "                \n",
    "            if true_class in score_list[3*i:3*i+3]:\n",
    "                auc3 += 1\n",
    "                neg_pos_list3.append(1)\n",
    "            else:\n",
    "                neg_pos_list3.append(0)\n",
    "\n",
    "        return auc, auc2/N, auc3/N, neg_pos_list1, neg_pos_list2, neg_pos_list3\n",
    "    \n",
    "    def result(self, epoch, time, loss, auc_dev,\n",
    "               auc_test, file_result):\n",
    "        with open(file_result, 'a') as f:\n",
    "            result = map(str, [epoch, time, loss, auc_dev, auc_test])\n",
    "            f.write('\\t'.join(result) + '\\n')\n",
    "\n",
    "    def save_model(self, model, file_name):\n",
    "        torch.save(model, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b222f",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a308bfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensor(file_name, dtype):\n",
    "    return [dtype(d).to(device) for d in np.load(file_name + '.npy', allow_pickle=True)]\n",
    "\n",
    "\n",
    "def load_numpy(file_name):\n",
    "    return np.load(file_name + '.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    with open(file_name, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def shuffle_dataset(dataset, seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, ratio):\n",
    "    n = int(ratio * len(dataset))\n",
    "    dataset_1, dataset_2 = dataset[:n], dataset[n:]\n",
    "    return dataset_1, dataset_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14959df",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df44dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "radius         = 2\n",
    "dim            = 50\n",
    "layer          = 2\n",
    "batch          = 10\n",
    "lr             = 1e-3\n",
    "lr_decay       = 0.5\n",
    "decay_interval = 10\n",
    "iteration      = 100\n",
    "extra_dim      = 20\n",
    "\n",
    "(dim, layer, batch, decay_interval, iteration, extra_dim) = map(int, [dim, layer, batch, decay_interval, iteration, extra_dim])\n",
    "lr, lr_decay = map(float, [lr, lr_decay])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5d3b5",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "800bf54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_input = ('inputgcn'+str(radius)+'/')\n",
    "\n",
    "molecules    = load_tensor(dir_input + 'molecules', torch.LongTensor)\n",
    "adjacencies  = load_numpy(dir_input + 'adjacencies')\n",
    "t_properties = load_tensor(dir_input + 'properties', torch.LongTensor)\n",
    "maccs        = load_numpy(dir_input + 'maccs')\n",
    "\n",
    "with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "    fingerprint_dict = pickle.load(f)\n",
    "    unknown          = 100\n",
    "    n_fingerprint    = len(fingerprint_dict) + unknown\n",
    "    \n",
    "dataset = list(zip(molecules, adjacencies, t_properties, maccs))\n",
    "dataset = shuffle_dataset(dataset, 1234)\n",
    "dataset_train, dataset_   = split_dataset(dataset, 0.8)\n",
    "dataset_dev, dataset_test = split_dataset(dataset_, 0.5)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "model   = MolecularPropertyPrediction().to(device)\n",
    "trainer = Trainer(model)\n",
    "tester  = Tester(model)\n",
    "\n",
    "dir_output = ('output/')\n",
    "os.makedirs(dir_output, exist_ok=True)\n",
    "\n",
    "dir_result = ('output/result/')\n",
    "os.makedirs(dir_result, exist_ok=True)\n",
    "\n",
    "dir_model  = ('output/model/')\n",
    "os.makedirs(dir_model, exist_ok=True)\n",
    "\n",
    "file_result = dir_result + '.txt'\n",
    "with open(file_result, 'w') as f:\n",
    "    f.write('Epoch\\tTime(sec)\\tLoss_train\\tAUC_dev\\tAUC_test\\n')\n",
    "\n",
    "file_model = dir_model + 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42acf61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch \t Time(sec) \t Loss_train \t AUC_dev \t AUC2_dev \t AUC3_dev \t AUC_test \t AUC2_test \t AUC3_test\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/THCBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-168ec6523304>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mauc_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc2_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc3_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mauc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc2_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc3_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn_nplist1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn_nplist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcn_nplist3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-d40d0ece81f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset_train)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1c9003f4e393>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data_batch, train, correlation_analysis)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mz_properties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msel_maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1c9003f4e393>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, sel_maccs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_atoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mextra_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msel_maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1c9003f4e393>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, xs, adjacency, i)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mhs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_atom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chem_gcn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chem_gcn/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/chem_gcn/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cublas runtime error : the GPU program failed to execute at /opt/conda/conda-bld/pytorch_1533672544752/work/aten/src/THC/THCBlas.cu:249"
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "print('Epoch \\t Time(sec) \\t Loss_train \\t AUC_dev \\t AUC2_dev \\t AUC3_dev \\t AUC_test \\t AUC2_test \\t AUC3_test')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "for epoch in range(iteration):\n",
    "    if (epoch+1) % decay_interval == 0:\n",
    "        trainer.optimizer.param_groups[0]['lr'] *= lr_decay\n",
    "\n",
    "    loss = trainer.train(dataset_train)\n",
    "    auc_dev, auc2_dev, auc3_dev, _, _, _ = tester.test(dataset_dev)\n",
    "    auc_test, auc2_test, auc3_test, gcn_nplist1, gcn_nplist2, gcn_nplist3 = tester.test(dataset_test)\n",
    "    \n",
    "    lr_rate = trainer.optimizer.param_groups[0]['lr']\n",
    "\n",
    "    end = timeit.default_timer()\n",
    "    time = end - start\n",
    "\n",
    "    tester.result(epoch, time, loss, auc_dev, auc_test, file_result)\n",
    "    tester.save_model(model, file_model)\n",
    "\n",
    "    print('%d \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f \\t %.4f' %(epoch, time, loss, auc_dev, auc2_dev, auc3_dev, auc_test, auc2_test, auc3_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3df6e",
   "metadata": {},
   "source": [
    "# Let's use Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa542d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_input = ('input'+str(radius)+'/')\n",
    "\n",
    "molecules    = load_tensor(dir_input + 'molecules', torch.LongTensor)\n",
    "adjacencies  = load_numpy(dir_input + 'adjacencies')\n",
    "t_properties = load_tensor(dir_input + 'properties', torch.LongTensor)\n",
    "maccs        = load_numpy(dir_input + 'maccs')\n",
    "\n",
    "with open(dir_input + 'fingerprint_dict.pickle', 'rb') as f:\n",
    "    fingerprint_dict = pickle.load(f)\n",
    "    \n",
    "dataset = list(zip(molecules, adjacencies, t_properties, maccs))\n",
    "dataset = shuffle_dataset(dataset, 1234)\n",
    "dataset_train, dataset_   = split_dataset(dataset, 0.8)\n",
    "dataset_dev, dataset_test = split_dataset(dataset_, 0.5)\n",
    "\n",
    "data_batch = list(zip(*dataset_train))\n",
    "properties_train, maccs_train = data_batch[-2], data_batch[-1]\n",
    "\n",
    "data_batch = list(zip(*dataset_dev))\n",
    "properties_dev, maccs_dev = data_batch[-2], data_batch[-1]\n",
    "\n",
    "data_batch = list(zip(*dataset_test))\n",
    "properties_test, maccs_test = data_batch[-2], data_batch[-1]\n",
    "\n",
    "train_len, dev_len, test_len = len(dataset_train), len(dataset_dev), len(dataset_test)\n",
    "\n",
    "feature_len = maccs_train[0].shape[0]\n",
    "\n",
    "X_train, X_dev, X_test = np.zeros((train_len,feature_len)), np.zeros((dev_len,feature_len)), np.zeros((test_len,feature_len))\n",
    "Y_train, Y_dev, Y_test = np.zeros((train_len,)), np.zeros((dev_len,)), np.zeros((test_len,))\n",
    "\n",
    "for i in range(train_len):\n",
    "    X_train[i,:] = maccs_train[i]\n",
    "    Y_train[i] = properties_train[i][0]\n",
    "    \n",
    "for i in range(dev_len):\n",
    "    X_dev[i,:]   = maccs_dev[i]\n",
    "    Y_dev[i]   = properties_dev[i][0]\n",
    "    \n",
    "for i in range(test_len):\n",
    "    X_test[i,:]  = maccs_test[i]\n",
    "    Y_test[i]  = properties_test[i][0]\n",
    "\n",
    "fingerprint_dict = load_pickle(dir_input + 'fingerprint_dict.pickle')\n",
    "unknown          = 100\n",
    "n_fingerprint    = len(fingerprint_dict) + unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5064c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples : 454, Top-1 Correct : 398, Top-1 Incorrect : 56\n",
      "Prediction Accuracy Top-1 : 87.67%\n",
      "Total samples : 454, Top-2 Correct : 433, Top-2 Incorrect : 21\n",
      "Prediction Accuracy Top-2 : 95.37%\n",
      "Total samples : 454, Top-3 Correct : 440, Top-3 Incorrect : 14\n",
      "Prediction Accuracy Top-3 : 96.92%\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=300, criterion = 'gini', max_depth=60, random_state=0)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "rf_neg_pos_list1 = []\n",
    "rf_neg_pos_list2 = []\n",
    "rf_neg_pos_list3 = []\n",
    "\n",
    "Y_pred = clf.predict_proba(X_test)\n",
    "\n",
    "num_classes = 3\n",
    "top_n = np.argsort(Y_pred)[:,:-num_classes-1:-1]\n",
    "top_n = clf.classes_[top_n]\n",
    "\n",
    "TP1 = 0\n",
    "TP2 = 0\n",
    "TP3 = 0\n",
    "\n",
    "FN1 = 0\n",
    "FN2 = 0\n",
    "FN3 = 0\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    true_class = Y_test[i]\n",
    "    \n",
    "    if true_class in top_n[i,0:1]:\n",
    "        TP1 += 1\n",
    "        rf_neg_pos_list1.append(1)\n",
    "    else:\n",
    "        FN1 += 1\n",
    "        rf_neg_pos_list1.append(0)\n",
    "    \n",
    "    if true_class in top_n[i,0:2]:\n",
    "        TP2 += 1\n",
    "        rf_neg_pos_list2.append(1)\n",
    "    else:\n",
    "        FN2 += 1\n",
    "        rf_neg_pos_list2.append(0)\n",
    "        \n",
    "    if true_class in top_n[i,0:3]:\n",
    "        TP3 += 1\n",
    "        rf_neg_pos_list3.append(1)\n",
    "    else:\n",
    "        FN3 += 1\n",
    "        rf_neg_pos_list3.append(0)\n",
    "print('Total samples : %d, Top-1 Correct : %d, Top-1 Incorrect : %d' %(TP1+FN1, TP1, FN1))\n",
    "print('Prediction Accuracy Top-1 : %.2f%%' %(100.0*TP1/(TP1+FN1)))\n",
    "        \n",
    "print('Total samples : %d, Top-2 Correct : %d, Top-2 Incorrect : %d' %(TP2+FN2, TP2, FN2))\n",
    "print('Prediction Accuracy Top-2 : %.2f%%' %(100.0*TP2/(TP2+FN2)))\n",
    "\n",
    "print('Total samples : %d, Top-3 Correct : %d, Top-3 Incorrect : %d' %(TP3+FN3, TP3, FN3))\n",
    "print('Prediction Accuracy Top-3 : %.2f%%' %(100.0*TP3/(TP3+FN3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem_gcn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
